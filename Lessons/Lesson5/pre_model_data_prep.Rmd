---
title: "Model prep + target_and_one_way"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Policy and Claims data exploration
As mentioned in previous lessons, we identified good and bad performers in term of portfolios. To identify, which group of people/companies are guilty for that performance we need **more granular data**. And so we received *Claims* and *Policy* data from out IT department.

Let's look at them.

#### Exercise 1
Please make a few data checks (fields info, some summary stats, quality checks about missings, ...) as you learned in previous lessons.

You can follow up auxiliary questions in `loss_analysis.R` at Exercise 1.

Write a few sentences about what have you found and commit your notes.



### Join Claims with Policy info
It is obvious we need to join both dataset together as __claims data__ contains info about __how much__ is the individual client risky. __Policy data__ contains info about individual's feature and other identifiers, which might help us to find reasons for __why__ client __is riskier__.

So let's find out the way how to join them together.

Policy data are at *policy + object* level.
```{r}
library(dplyr)
dt_Policy <- read.csv("C:\\Users\\Matej\\Documents\\github\\GeneralInsurance_Class\\Data\\lesson5_PolicyHistory.csv") %>% distinct(NrPolicy, NrObject, .keep_all = TRUE) 
dt_Policy %>% nrow()
dt_Policy %>% distinct(NrPolicy, NrObject) %>% nrow() 
# budeme chciet najst cenu poistneho pre konkretneho cloveka...
```

Claims data are at *claims* level. 
```{r}
dt_Claims <- read.csv("C:\\Users\\Matej\\Documents\\github\\GeneralInsurance_Class\\Data\\lesson5_Claims.csv") %>% distinct(NrClaim, .keep_all = TRUE)
dt_Claims %>% nrow()
dt_Claims %>% distinct(NrClaim) %>% nrow() # nie su duplicity
```

To properly bring the claims info on `policy + object` level we need to ensure claims data are also on that level. Lets check it and if they are not, roll them up to that level. 

```{r}
dt_Claims %>% distinct(NrPolicy, NrObject) %>% nrow()
## they are on required level as no. of unique rows at level equals to no. of rows for raw dataset, 
## if they wouldn't this is how you would roll it up
# dt_Claims %>% 
#   mutate(clm_yr = lubridate::year(Date_of_loss)) %>% 
#   group_by(NrPolicy, NrObject, clm_yr) %>% 
#   summarise_at(vars(Paid, Reserves), funs(sum), na.rm = TRUE)
```

Cool! Lets finally join them together.
```{r}
dt_pol_w_claims <- left_join(dt_Policy, 
                             dt_Claims, 
                             by = c("NrPolicy", "NrObject")
                    )
head(dt_pol_w_claims)
```

Perfect! So what's now? Hmm...

Let's look how succesfull we have been to find out about our client losses.

### Quality Assurance of Join
```{r}
# how much claims we joined?
dt_pol_w_claims %>% group_by(is.na(Paid)) %>% summarise(cnt = n())
# 2100 vyplatenych skod, 28217 bez skody
# niektore z tych 4349 sme teda zahodili v join-e, kedze ich je uz len 2100
```

Does it look good or bad? Two questions should arise in your head:

  - Is there any better way to join those information together? Am I missing anything? Does the IT provide good data pull?
  - Lets suppose everything looks good in your Join. Does it make sense to you to have such amount of claims for portfolio you analysing? If you are not sure, you should ask your business or claims department.

# Looking for good Target

#### ...and what is the Target?
Target is a __dependent__ variable in a model, you designed for the purpose of being helpful in predicting the insurance event. Good design of the model can answer questions about relationship between any __independent__ variable and target. When we spoke about _variable_ it can be anything which is __measurable__ (event, object, facts, idea, ...).

Questions, which might help you identify good target and design good model:

- What is the event, object or idea you would like to predict?
- Is it measurable?
- Let's say you finished your model, how did the prediction help you to solve your insurance issue?
- Are there any potential independent variables, they might have relationship to your target you propose?

> Now the crucial questions: What are you gonna model and what data will you need for that purpose?



Write a few sentences about what do you think and commit your notes.



#### Exercise 2
Go to Exercise 2 and try to think about what could be a good 'target'/ event you are trying to predict from data you have.


Great, now you wrote about your ideas about the target, lets think about it together.

- _What is the event, object or idea you would like to predict?_

    We definitely want to find out the risk of the specific client and predict it for potential clients.
   
- _Is it measurable?_

    How can we measure risk? Which information identify risky client? 
   
    This is something we need to discuss more and have whole section about it. Example of good indicator for risk can be Ultimate Losses, Ultimate Loss Ratio, Ultimate Frequency and depend pretty much on which kind of risk you are interested in. 
   
     Lets take an example. If you are really interested in risk described in dollar values, you will probably end up using Ultimate Loss as good measurable variable indicating risk of the client.
   
    But, some of the insurance coverages pay only fixed amount of dollars when claims occurs. In such case there is not so much importance on dollar value paid, but much more important thing is how much claims such client had. We usually choose Ultimate Frequency as a good target or combination of Loss amount and Frequency unit.
   
    So the answer here depends pretty much on the definition of the insurance product and it's specific risk you are trying to predict.


So once again...

> What is a good measure of risky client?

Hmm. `Paid` losses might be a good start.

```{r}
dt_pol_w_claims %>% 
  filter(!is.na(Paid)) %>% 
  select(Paid) %>% 
  arrange(desc(Paid)) %>% 
  head()
```

### Exposure
> Is it enough? Can we say, those first three client have similar risk?

Well...it might be not enough. Lets look to the same example, but we will add some more information. 
Specifically, we will show when the policy started and when it was ended.

```{r}
dt_pol_w_claims %>% 
  filter(!is.na(Paid)) %>% 
  select(Paid, Dt_Exp_Start, Dt_Exp_End) %>% 
  arrange(desc(Paid)) %>% 
  head()
```


> What's different on those first three clients?

The third client asked for insurance cover only for three months! And during those three months they have similar loss amount as other clients have during one year. This leads to redefining the risk of the third client to be 4-times(!) riskier than first two clients from the table above.

What we described here is term `exposure`. The exposure can be different for portfolio we are analysing. e.g. it can be square root for property business or mileage for trucks insurance. We often talks about exposure as unit of for insurance cover. There is many definitions. 

So let's create our exposure based on time, client was covered.
```{r}
# tip: there is a great package for date manipulation called lubridate
library(lubridate)
dt_pol_w_claims <- 
  dt_pol_w_claims %>% mutate(Time_Exposure = lubridate::dmy(Dt_Exp_End) - lubridate::dmy(Dt_Exp_Start))

# same example as above with Time Exposure
dt_pol_w_claims %>% 
  filter(!is.na(Paid)) %>% 
  select(Paid, Dt_Exp_Start, Dt_Exp_End, Time_Exposure)  %>% 
  arrange(desc(Paid)) %>% head()

# cislo 3 je 4-6 krat rizikkovejsi, kedze stihol naburat len za 68 dni

# frekvencia = #claims/sum(exposure)
# SEV = sum(claims)/#claims ...severita
# pocitame nalady na poistku na jeden den... vynasobime frekvenciu a SEV
# dostaneme BC = sum(claims)/sum(exposure)  ... burning costs (priemerne skody) ... toto budeme modelovat
```

Did you realize for some years there is 365 and for some 366 days? Cool, right? `lubridate` does know which year is a leap year.

#### Ultimate Losses and Burning Cost
Ultimate Losses is something we end up paying overall for the individual claim. It can include various parts of the claim e.g. (Losses, Reserver, Inflation, Expenses to arrange the claim, ...).

Burning Cost we call overall cost per some metric of exposure, in our case we are talking about _dollar loss per day insured_.

```{r}
dt_pol_w_claims <- 
  dt_pol_w_claims %>% 
  mutate(Ult_Loss = Paid + Reserves,
         Burning_Cost = ifelse(is.na(Ult_Loss), 0,  Ult_Loss / as.integer(Time_Exposure))
  )

dt_pol_w_claims %>% 
  filter(!is.na(Paid)) %>% 
  select(Paid, Reserves, Ult_Loss, Burning_Cost) %>% head()
```

Let's continue with the questions which help us to identify a good target.

- _Let's say you finished your model, how did the prediction help you to solve your insurance issue?_

    When we will have a good model, which could predict Ultimate Time weighted Loss accuratelly, it will help us to build suitable price we should offer to potential client. The price should ensure it is not too high for less risky client, so we are competitive on the market, but also high enough to cover for potential claims. Btw we might call such a price as _Technical Price_.

- _Are there any potential independent variables, they might have relationship to your target you propose?_

    This is something we can solve using _One-Way Analysis_

## One-Way Analysis
Perfect! It looks like we have found a good target, which might be a good measure for risky clients.

Now, lets finally try to think about the __reasons__ of client being more risky than others.

#### Exercise
Do you have any idea which type of client is definitelly riskier than other? 
Write a few sentences to your notes and commit.


--------------------------------------------------------------------------------

One-Way analysis means we always look for one explanatory variable and one which we try to explain, in our case it's our target we identified. So first of all it make sense to look into them as basic _scatterplot_.

For the first one-way analysis we will try to explore feature about vehicle type of client: `Veh_type2`

```{r}
library(ggplot2)
dt_pol_w_claims %>% 
  ggplot(aes(y = Burning_Cost, x = Veh_type2)) + 
  geom_jitter()
```


Does it helps you to identify any trend? Hmm...looks like outliers screwing it up. Lets go for numbers then.

```{r}
dt_pol_w_claims %>% 
  group_by(Veh_type2) %>% 
  summarise(BC_avg = mean(Burning_Cost, na.rm  = TRUE),
            BC_median = median(Burning_Cost, na.rm = TRUE),
            cnt = n()) %>% 
  arrange(desc(BC_avg))
```

Why we choose those three metrics? And do you see the story behind them?

```{r}
dt_pol_w_claims %>% 
  ggplot(aes(y = Burning_Cost, x = Veh_type2)) + 
  geom_boxplot() +
  ylim(0, 100)

#hist(dt_pol_w_claims$Paid, breaks = 100)
```


Two things happend here:

    - outliers screw it up so much, that this feature is definitelly not good predictor alone, we are not able to explain those outliers using only one feature describing vehicle type.
    - but...there is definitelly some trend, which might be usefull in next stages of modelling. (saying that there is definitelly some trend might be too strong and we should use also other methods to confirm this. e.g. ANOVA)

#### Exercise
Choose another feature and try to find out the story behind data, using similar approach as above.



#### Basic GLM

We created Target and analysed it with One-Way approach. Now, let's see how it performs with a basic glm.

### Distribution of the Target
Before we jump directly to modelling, we need to decide on which type of GLM model we can actually use.
The first step in decision-making could be to draw a distribution of our Target.

```{r}
library(ggplot2)
ggplot(data = dt_pol_w_claims,
                aes(x = Burning_Cost)) +
geom_histogram()
```
It does not look to be very usefull as there is a lot of clients that don't have any claim. Let's remove them, and we can remove outliers as well.

```{r}
library(ggplot2)
ggplot(data = dt_pol_w_claims %>% filter(Burning_Cost != 0, Burning_Cost < 100),
                aes(x = Burning_Cost)) +
geom_histogram()
```

What type of distribution it reminds you of? It looks like `Gamma` or `Tweedie` (if you have never heard about Tweedie distribution, it is a combination of Poisson and Gamma, check wiki for more info). 

To make a proper decision we would run some statistical test on matching distribution, e.g. Kolmogorov-Smirnov test and similar.


So lets try `Gamma` as a first attempt.
### First Model
```{r}
model1 <- glm(data = dt_pol_w_claims %>% filter(Burning_Cost != 0, Burning_Cost < 100),
              formula = Burning_Cost ~ Veh_type2,
              family = Gamma())
# formula = Burning_Cost ~ Veh_type2 + age,
# ak vsetky premenne v modeli, tak '.'
# ak pocet Claimov napriklad, tak skor Poisson ako Gamma ... (pri diskretnych skor Poisson)
# large Claim a small Claim mozu mat ine rozdelenia, tu take nieco vsak uvazovat nebudeme... podla chvostov rozdeleni (long tail, short tail)
```

```{r}
summary(model1)
```

